import os
import logging
from openai import AsyncOpenAI
from dotenv import load_dotenv

# 1. Åadowanie zmiennych Å›rodowiskowych (klucza API)
load_dotenv()

# Konfiguracja loggera, Å¼eby widzieÄ‡ co siÄ™ dzieje w konsoli
logger = logging.getLogger("newsroom_llm")
logging.basicConfig(level=logging.INFO)

# 2. Lazy initialization - klient tworzony dopiero przy pierwszym uÅ¼yciu
_client: AsyncOpenAI | None = None


def _get_client() -> AsyncOpenAI:
    """Zwraca klienta GitHub Models (lazy initialization)."""
    global _client
    if _client is None:
        api_key = os.environ.get("GITHUB_TOKEN")
        if not api_key:
            raise ValueError("Brak GITHUB_TOKEN w zmiennych Å›rodowiskowych! SprawdÅº plik .env")
        _client = AsyncOpenAI(
            base_url="https://models.inference.ai.azure.com",
            api_key=api_key,
        )
    return _client

async def get_completion(
    user_prompt: str, 
    system_prompt: str = "JesteÅ› pomocnym asystentem AI.", 
    model: str = "gpt-4o-mini"
) -> str:
    """
    WysyÅ‚a zapytanie do modelu LLM (GitHub Models) i zwraca treÅ›Ä‡ odpowiedzi.
    
    Args:
        user_prompt (str): TreÅ›Ä‡ zadania dla agenta.
        system_prompt (str): Rola agenta (np. "JesteÅ› surowym redaktorem").
        model (str): Nazwa modelu (domyÅ›lnie gpt-4o-mini lub Llama-3.1).
        
    Returns:
        str: OdpowiedÅº modelu.
    """
    try:
        logger.info(f"ğŸ¤– WysyÅ‚anie zapytania do modelu: {model}...")
        
        response = await _get_client().chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.7, # KreatywnoÅ›Ä‡ (0.0 = robot, 1.0 = artysta)
        )
        
        content = response.choices[0].message.content
        logger.info("âœ… Otrzymano odpowiedÅº.")
        return content

    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d podczas komunikacji z LLM: {e}")
        # W przypadku bÅ‚Ä™du zwracamy pusty string lub komunikat, 
        # Å¼eby program siÄ™ nie wywaliÅ‚ caÅ‚kowicie (zgodnie z planem obsÅ‚ugi bÅ‚Ä™dÃ³w).
        return f"ERROR: Nie udaÅ‚o siÄ™ uzyskaÄ‡ odpowiedzi. PowÃ³d: {str(e)}"

# --- Sekcja testowa (uruchomi siÄ™ tylko przy bezpoÅ›rednim wywoÅ‚aniu pliku) ---
if __name__ == "__main__":
    import asyncio

    async def test_run():
        print("Testowanie poÅ‚Ä…czenia z GitHub Models...")
        answer = await get_completion(
            user_prompt="Opowiedz krÃ³tki Å¼art o programistach.",
            system_prompt="JesteÅ› komikiem."
        )
        print(f"\nOdpowiedÅº modelu:\n{answer}")

    asyncio.run(test_run())